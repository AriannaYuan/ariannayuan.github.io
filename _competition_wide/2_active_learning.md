---
title: Building machines that learn visual concepts like humans (2018)
image: /images/competitions_wide/active_learning.png
description: Current deep learning models in computer vision are extremely data-hungry. In addition, they are often passive learners. Because deep learning models usually do not explicitly represent the structural information in natural images, they often cannot abstract high-level information away from raw images as humans do which enables cross-domain transfer. To address the issue of the absence of active learning, we use the policy gradient method to train a deep reinforcement learning (RL) agent that can help a visual model to actively select training examples. We further explore whether this active learning skill could generalize to unseen classes (within-domain generalization) in the same domain and novel tasks in a different domain (cross-domain transfer). In addition, we compare the model performance when using object segmentation map as inputs for the RL agent with the performance when using the raw pixels as inputs. We show that only when segmentation map is used, active learning demonstrates a benefit over the baseline, confirming the hypothesis that active learning requires explicit representation of the structural information of images.
link : https://github.com/AriannaYuan/ariannayuan.github.io/raw/master/pdf/None.pdf
---
